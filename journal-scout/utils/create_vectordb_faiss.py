#!/usr/bin/env python3
"""
Vector Database Builder using FAISS + Sentence Transformers
Generated by Journal Scout

Requirements:
  pip install faiss-cpu sentence-transformers

Usage:
  python create_vectordb.py
"""

import os
import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# Configuration
PAPERS_DIR = "./papers"
DB_NAME = "papers_vectordb"

def main():
    print("Loading embedding model...")
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    print(f"Processing papers from {PAPERS_DIR}...")
    documents = []
    metadatas = []
    
    for i, filename in enumerate(os.listdir(PAPERS_DIR)):
        if filename.endswith(('.txt', '.md')):
            filepath = os.path.join(PAPERS_DIR, filename)
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            title = filename
            if 'TITLE:' in content:
                title = content.split('TITLE:')[1].split('\n')[0].strip()
            
            documents.append(content)
            metadatas.append({"filename": filename, "title": title, "index": i})
            print(f"  Loaded: {filename}")
    
    if not documents:
        print("No .txt or .md files found!")
        return
    
    print("\nGenerating embeddings...")
    embeddings = model.encode(documents, show_progress_bar=True)
    embeddings = np.array(embeddings).astype('float32')
    
    # Create FAISS index
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatIP(dimension)  # Inner product (cosine with normalized vectors)
    faiss.normalize_L2(embeddings)  # Normalize for cosine similarity
    index.add(embeddings)
    
    # Save index
    faiss.write_index(index, f"{DB_NAME}.faiss")
    
    # Save metadata
    with open(f"{DB_NAME}_metadata.json", 'w') as f:
        json.dump({"documents": documents, "metadatas": metadatas}, f)
    
    print(f"\nâœ… FAISS index created with {len(documents)} documents!")
    print(f"   Index: {DB_NAME}.faiss")
    print(f"   Metadata: {DB_NAME}_metadata.json")
    
    # Export for Journal Scout
    export_data = {
        "name": DB_NAME,
        "createdAt": __import__('datetime').datetime.now().isoformat(),
        "dimension": dimension,
        "documents": [
            {
                "id": f"doc_{i}",
                "fileName": metadatas[i]["filename"],
                "content": documents[i],
                "embedding": embeddings[i].tolist(),
                "metadata": {"title": metadatas[i]["title"], "createdAt": __import__('datetime').datetime.now().isoformat()}
            }
            for i in range(len(documents))
        ]
    }
    
    with open(f"{DB_NAME}_export.json", 'w') as f:
        json.dump(export_data, f)
    print(f"   Exported: {DB_NAME}_export.json")

if __name__ == "__main__":
    main()
