#!/usr/bin/env python3
"""
Vector Database Builder using ChromaDB + Sentence Transformers
Generated by Journal Scout

Requirements:
  pip install chromadb sentence-transformers

Usage:
  python create_vectordb.py
"""

import os
import json
import chromadb
from sentence_transformers import SentenceTransformer

# Configuration
PAPERS_DIR = "./papers"  # Folder with your .txt files
DB_DIR = "./vectordb"    # Where to store the vector database
COLLECTION_NAME = "papers_vectordb"

def main():
    # Initialize embedding model (runs locally, no API needed)
    print("Loading embedding model...")
    model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast & good quality
    # Alternative: 'all-mpnet-base-v2' for better quality
    
    # Initialize ChromaDB
    print("Initializing ChromaDB...")
    client = chromadb.PersistentClient(path=DB_DIR)
    
    # Create or get collection
    collection = client.get_or_create_collection(
        name=COLLECTION_NAME,
        metadata={"hnsw:space": "cosine"}
    )
    
    # Load and process papers
    print(f"Processing papers from {PAPERS_DIR}...")
    documents = []
    metadatas = []
    ids = []
    
    for i, filename in enumerate(os.listdir(PAPERS_DIR)):
        if filename.endswith(('.txt', '.md')):
            filepath = os.path.join(PAPERS_DIR, filename)
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Extract title if present
            title = filename
            if 'TITLE:' in content:
                title = content.split('TITLE:')[1].split('\n')[0].strip()
            
            documents.append(content)
            metadatas.append({"filename": filename, "title": title})
            ids.append(f"doc_{i}")
            print(f"  Loaded: {filename}")
    
    if not documents:
        print("No .txt or .md files found!")
        return
    
    # Generate embeddings and add to collection
    print("\nGenerating embeddings...")
    embeddings = model.encode(documents, show_progress_bar=True)
    
    collection.add(
        documents=documents,
        embeddings=embeddings.tolist(),
        metadatas=metadatas,
        ids=ids
    )
    
    print(f"\nâœ… Vector database created with {len(documents)} documents!")
    print(f"   Location: {DB_DIR}")
    
    # Export for Journal Scout import
    export_data = {
        "name": COLLECTION_NAME,
        "createdAt": __import__('datetime').datetime.now().isoformat(),
        "dimension": len(embeddings[0]),
        "documents": [
            {
                "id": ids[i],
                "fileName": metadatas[i]["filename"],
                "content": documents[i],
                "embedding": embeddings[i].tolist(),
                "metadata": {"title": metadatas[i]["title"], "createdAt": __import__('datetime').datetime.now().isoformat()}
            }
            for i in range(len(documents))
        ]
    }
    
    with open(f"{COLLECTION_NAME}_export.json", 'w') as f:
        json.dump(export_data, f)
    print(f"   Exported: {COLLECTION_NAME}_export.json (for Journal Scout import)")

if __name__ == "__main__":
    main()
